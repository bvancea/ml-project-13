\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Machine Learning 2013: Project 1 - Regression Report}
\author{altrif@student.ethz.ch\\ bvancea@student.ethz.ch\\ member3@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section*{Experimental Protocol}
\section{Tools}

For the project we used to following tools:
    \begin{itemize}
        \item The Python programming language
        \item The Scikit-learn Python library. We used this library for preprocessing data and prediction.
        \item The Pandas Python library. We used this library for parsing csv files and data management.
        \item The py-earth Python libary. We used this library for a open-source MARS implementation.
    \end{itemize}

\section{Algorithm}
We tried several models for the regression task, however the model that performed the best was a MARS (Multivariate Adaptive Regression Splines) model. This type of model was not described in the lectures and we will briefly describe the approach used. 
The algorithm constructs a piecewise-linear model of the form:
\begin{equation}
    \hat{f}{(x)} = \sum_{i=1}^{k}{c_iB_i(x)}
\end{equation}
In this model, $c_i$ represent constants and $B_i$ represent Hinge functions of the form $max(0,x-c)$ or $max(x-c,0)$. The pair of function presented is called a \emph{reflected pair}. It is possible to model a basis function as a product of two basis function and therefore increasing the \emph{degree of interaction} in the model. The MARS algorithm consistens of two steps:
\begin{enumerate}
\item[The forward pass] Starting for a single constant function, the algorithm iteratively adds the pair of Hinge functions that decreases the sum-of-squares error.
\item[Backward pass] Due to the fact that the \textbf{forward pass} uses a greedy procedure \emph{greedy}, this pass prunes the model of the least-effective terms (according to the GCV (Generalized Cross-Validation) criterion).
\end{enumerate}
Because of the nature of the algorithm used MARS provides both regularization and feature selection.

\section{Features}
Our methodology for contructing new features is the following:  
\begin{enumerate}
    \item We constructed the correlation matrix for the training set. This includes both the feature vectors X and the response vector y.
    \item We applied several transformation to features which present the strongest correlation with the y vector. More specifically, for all the features for which the value of the correlation was greater than a $\epsilon$, we added new features consiting of $sin(x)$, $log(x)$, $x^2$ etc.
    \item We also took in consideration that features could influence one another. Because of this, we also added new features corresponding to combinations of two highly correlated features. Therefore, for all pairs of features $x_1$, $x_2$, we added a new feature corresponding to $sin(x_1) + sin(x_2)$.
    
\end{enumerate}

\section{Parameters}

For the MARS model, the parameter we needed to choose was the maximum degree of interaction betweem the basis functions generated by the model. We obtained the best degree through k-Fold cross-validation over a range of possible degrees. We repeated the procedures for several values of k: 2,3,5,10 and picked the best performing model. In our experiments, a MARS model with the maximum degree of 2 performed the best in all the cases.

\section{Lessons Learned}
We also tried to train several other regression models:
\begin{itemize}
    \item A simple least-squares linear regressor. This model performed the worst of all the models we tried. The main reason for this is the even the with the added features we did not obtain a linear relationship. 
    \item A $L_2$ regularized least-squares regression (Ridge). This model performed slighly better than the least-squares model, however it still did not go over the hard baseline. We tried several values of $\alpha$ using cross-validation, but we think that even with the regularization, we did not obtain a linear relationship.
    \item A $L_1$ regularized least-square regression (Lasso). Our main assumption was that we might not obtain a robust model due to the fact that we may have added more features than necessary. We attempted to regularize using the $L_2$ norm hoping to obtain a sparser model containing only the most relevant features.
\end{itemize}

\end{document} 
